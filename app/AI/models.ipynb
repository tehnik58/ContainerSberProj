{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:51: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\maksi\\AppData\\Local\\Temp\\ipykernel_8896\\3186870174.py:51: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  return [int(x) for x in re.findall(\"\\d+\", st)]\n"
     ]
    }
   ],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, path, path2image, n_classes = 15):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        self.dict = data\n",
    "        self.path2image = Path(path2image)\n",
    "        self.N = n_classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "\n",
    "    def _encode_label(self, classes):\n",
    "        label = np.zeros(self.N)\n",
    "        for c in np.array(classes, dtype = np.int16) - 1:\n",
    "            label[c] = 1\n",
    "        return label\n",
    "        \n",
    "    def sample(self, clss = 1, count = 15):\n",
    "        files = []\n",
    "        while len(files) < count:\n",
    "            indx = np.random.randint(0, self.__len__())\n",
    "            labels = self.dict[indx][\"OUTPUT:classes\"]\n",
    "            try:\n",
    "                if clss in self.parse(labels):\n",
    "                    files.append(self.path2image / self.file_name(indx))\n",
    "            except:\n",
    "                continue\n",
    "        return files\n",
    "    \n",
    "    \n",
    "    def create_path(self, indx):\n",
    "        return str(self.path2image / self.file_name(indx))\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        img = cv2.imread(self.create_path(indx))\n",
    "        clases = self.parse(self.dict[indx][\"OUTPUT:classes\"])\n",
    "        label = self._encode_label(clases)\n",
    "        sample = {'image': img,\n",
    "                  'label': label}\n",
    "        return sample\n",
    "            \n",
    "    def file_name(self, indx):\n",
    "        return self.dict[indx]['file_name']\n",
    "    \n",
    "    def resize(self, img, scale=2):    \n",
    "        shape = (np.array(img.shape[:2])/scale).astype(np.int32)\n",
    "        return cv2.resize(img,shape)\n",
    "    \n",
    "    \n",
    "    def parse(self, st):        \n",
    "        return [int(x) for x in re.findall(\"\\d+\", st)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchImageDataset(ImageDataset, Dataset):\n",
    "    def __init__(self, path, path2image, imgsz=256, MEAN = (0.485, 0.456, 0.406), STD = (0.229, 0.224, 0.225)):\n",
    "        super().__init__(path, path2image)\n",
    "        self.imgsz = imgsz\n",
    "        self.fransform_img= transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((self.imgsz, self.imgsz)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(10),    \n",
    "                    transforms.ColorJitter(brightness=0.2, \n",
    "                                                   contrast=0.2, \n",
    "                                                   saturation=0.2, \n",
    "                                                   hue=0.2),\n",
    "                    transforms.Normalize(mean=MEAN, std=STD),\n",
    "        ])\n",
    "            \n",
    "    @lru_cache(10000)\n",
    "    def __getitem__(self,indx):\n",
    "        img, label = super().__getitem__(indx).values()\n",
    "        if img is not None:\n",
    "            img = self.fransform_img(img)\n",
    "            return {'image':img,'label':label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "C:\\Users\\maksi\\AppData\\Local\\Temp\\ipykernel_8896\\1301915487.py:28: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if 'file_name' not in item or 'OUTPUT:classes' is None:\n"
     ]
    }
   ],
   "source": [
    "def merge_json_files(directory, output_file):\n",
    "    merged_data = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            with open(filepath, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    if is_valid_item(item):\n",
    "                        merged_data.append(item)\n",
    "\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    \n",
    "    for item in merged_data:\n",
    "        serialized_item = json.dumps(item, sort_keys=True)\n",
    "        if serialized_item not in seen:\n",
    "            seen.add(serialized_item)\n",
    "            unique_data.append(item)\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(unique_data, outfile, indent=4)\n",
    "\n",
    "def is_valid_item(item):\n",
    "    if 'file_name' not in item or 'OUTPUT:classes' is None:\n",
    "        print(f\"Skipping invalid item: {item}\")\n",
    "        return False  \n",
    "\n",
    "    classes = item.get('OUTPUT:classes', None)\n",
    "    \n",
    "    if classes is None:  # Пропускаем, если classes == None (или JSON null)\n",
    "        print(f\"Skipping item with invalid classes (None/null): {item}\")\n",
    "        return False \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping item with invalid classes (None/null): {'task_id': '1df19709-dc7a-448b-a5f2-61a8f65d4853', 'task_type': 'PROD', 'file_name': '35TimePhoto_20241001_063231 (54).jpg', 'organization_id': 'e2f6ea8c-516a-4a43-aaa1-0544a9a60ec9', 'person_id': 'bc52b760-0a6a-47e1-bb86-c01f0e931d87', 'item_id': 'c8323992-6b92-43dd-8996-e199afff29a9', 'item_type': 'data', 'assignment_id': 'b4816cb6-1919-489a-b0d3-9682cdebd2ae', 'status': 'ACCEPTED', 'marker_id': '3a933646-2de5-486d-8dc0-dacea62c685a', 'start_date': '2024-11-13T20:17:42.410395+03:00', 'end_date': '2024-11-13T20:18:31.125022+03:00', 'control_passed': None, 'pool_ids': '[\"bb899827-7127-4aaf-81db-531ffdbddea6\"]', 'price': 5.0, 'consistency': 0.0, 'quality': None, 'submitted_at': '2024-11-13T20:18:31.125022+03:00', 'skipped_at': None, 'expired_at': None, 'accepted_at': '2024-11-13T20:18:31.125022+03:00', 'rejected_at': None, 'started_at': '2024-11-13T20:17:42.410395+03:00', 'reviewer_id': None, 'reviewer_comment': None, 'OUTPUT:image': '/api/v0/storage/file/eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJhc3NpZ25tZW50X2lkIjoiYjQ4MTZjYjYtMTkxOS00ODlhLWIwZDMtOTY4MmNkZWJkMmFlIiwiYnVja2V0X2lkIjoiMWRmMTk3MDktZGM3YS00NDhiLWE1ZjItNjFhOGY2NWQ0ODUzIiwiZXhwIjoxNzMxNTE5MTYyLCJmaWxlX2lkIjoiYzgzMjM5OTItNmI5Mi00M2RkLTg5OTYtZTE5OWFmZmYyOWE5IiwibWFya2VyX2lkIjoiM2E5MzM2NDYtMmRlNS00ODZkLThkYzAtZGFjZWE2MmM2ODVhIiwibWF4X2F0dGFjaG1lbnRzX3Blcl9hc3NpZ25tZW50Ijo1MH0.NjdJhaS_Ea9weIX8VTjMaXAKIbkJ0qWxQG45c36qIyQgpVQUDhlBTobwZQvDWhLwriCqJj4L_yd90bOj8YGjkTqc228xmmxLYkP0br3P9kAkBfZcMAbUSajAn48sjGSk3vzYwTjs7J2C1T9hAgGHLTkLm9iBQKovGoNT9N9wlLBjqTjGRIfdTR9P-eXufllmGuulSl9XLiYkjZDX05Hnmoe2Zc_3MtQ2-IoJxRMD-s9EjTF8zDYsLx3yu6-HVQro12r2Lp2NuJ9IHl_a2okmhglbLsixYlhVy01JmeDh9pjwssGjD0I-MGpgMHt4orDJ2ekvXkdladpQK2PZ-4MHgk5NBqeLhfQLgeuCQw8O3UNJxABenK-JOBvjyTImblVNIpLY06z6Vdvep5LXTt_rwbu2BWMWsB6-M-TKVpmul0ehK7gwlkM7SPQklz1eJkd69OjkTLD7Y0J60Xy0wGE1nta9qbaZiSyEMcQB84gZXDDapvEA6QZ_nmexRZu23AM_vComrHrxuXJCnSULFo2ovftj6C7sqjflYIj6pGsBEEzY6OEuFsA6gjrWgaWRyaNTEW60ChN8YRRyH6yMS4AbRLHU_17HZJcAnjikJwHBArj1Nujvp-ElYQhJD-MftafXS-jVIcP4khGGqfPg4r6nM3vKUNd6AczwyTv6nHt_zrs', 'OUTPUT:classes': None}\n",
      "Skipping item with invalid classes (None/null): {'task_id': '78fa8015-2a27-482b-854e-58dadfa0ff7e', 'task_type': 'PROD', 'file_name': '46TimePhoto_20241020_061426 (62).jpg', 'organization_id': 'e2f6ea8c-516a-4a43-aaa1-0544a9a60ec9', 'person_id': 'bc52b760-0a6a-47e1-bb86-c01f0e931d87', 'item_id': 'c4e5d633-f9be-4de6-b265-2222173bf681', 'item_type': 'data', 'assignment_id': '3eee81f8-af2e-45ae-8b6c-2df02502d4f1', 'status': 'ACCEPTED', 'marker_id': '3e2e2c95-0d10-453f-a8e8-84b890346a54', 'start_date': '2024-11-15T12:57:48.768798+03:00', 'end_date': '2024-11-15T12:57:54.488799+03:00', 'control_passed': None, 'pool_ids': '[\"2b05ce6b-b1d4-4e20-a097-e0ed09a8ad01\"]', 'price': 3.667, 'consistency': 0.0, 'quality': None, 'submitted_at': '2024-11-15T12:57:54.488799+03:00', 'skipped_at': None, 'expired_at': None, 'accepted_at': '2024-11-15T12:57:54.488799+03:00', 'rejected_at': None, 'started_at': '2024-11-15T12:57:48.768798+03:00', 'reviewer_id': None, 'reviewer_comment': None, 'OUTPUT:image': '/api/v0/storage/file/eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJhc3NpZ25tZW50X2lkIjoiM2VlZTgxZjgtYWYyZS00NWFlLThiNmMtMmRmMDI1MDJkNGYxIiwiYnVja2V0X2lkIjoiNzhmYTgwMTUtMmEyNy00ODJiLTg1NGUtNThkYWRmYTBmZjdlIiwiZXhwIjoxNzMxNjY1NTY4LCJmaWxlX2lkIjoiYzRlNWQ2MzMtZjliZS00ZGU2LWIyNjUtMjIyMjE3M2JmNjgxIiwibWFya2VyX2lkIjoiM2UyZTJjOTUtMGQxMC00NTNmLWE4ZTgtODRiODkwMzQ2YTU0IiwibWF4X2F0dGFjaG1lbnRzX3Blcl9hc3NpZ25tZW50Ijo1MH0.E2hKejF1964AEMfr_O0R4hdLWF1p1s_L1kLZkAnRaLFGPJ_hSG-l7FfRByrRUNqL4hxBBEc8xy3c0oQLxbz4p3F5T_QROYw89PcbXq1zCusq3v7dtMswF9pG9_vVJOq3dqFsG6epcX7OvFl6gZmsNk6VrlyqWETHxR4q3N5ktb-Dajk38AbSp2ncz7-nkWx_QNV3WHayyBHSzQuXcpD1iJiFBnnu9DVccgHXL7yHZlJdxEEE7lkDRE8E29K-TzP-a2iZnjQPlSZVMchapm-qWrDS04OcjyLIwteYFXzZGUlbhnIMzc66Q7D_8o8e1t2tzriqOGu-gNt8kVnHn96Q8170_VI8ZapotL3C-I4e3P6xLVspjdjdpqnPYdLEo-rUkz4MIE0UbrIBDosRMO5xpwr9le0woTWj1mCCGSbZyw4egeIbhgEllK68wFQK9o_a1OHxgMSIO1CAeHfd-UrMQ9uAKb2B8EA7VDA4zYf6NY0dx9abYgS8yZIivIXWPs-XnV1dvApyslD3SeNvOJjhaZEb1QV-OxIma2oWdVzbRud8g5MIxDhaO8KibpRoSLVM_Vq6NhWkl8C6vdUhge5MuH8Ui2i4aHaHAztpWfKSmbGuwdYbajc1IZaSDkbWK_vW6IlBt17-bvR8WxpOod1kxynufMFrTlqecqfsB_j84oM', 'OUTPUT:classes': None}\n",
      "Skipping item with invalid classes (None/null): {'task_id': 'dbedb5bb-16bc-4e07-9b90-a885c86336ae', 'task_type': 'PROD', 'file_name': '58TimePhoto_20241006_062415 (33).jpg', 'organization_id': 'e2f6ea8c-516a-4a43-aaa1-0544a9a60ec9', 'person_id': 'bc52b760-0a6a-47e1-bb86-c01f0e931d87', 'item_id': '7f7ef318-697d-4bbe-b917-5451c059b493', 'item_type': 'data', 'assignment_id': '56b556b5-f12a-445c-84ce-008562d0e684', 'status': 'ACCEPTED', 'marker_id': '6af0237f-b6ce-4ef8-a0ea-6759a3aa5282', 'start_date': '2024-11-18T17:22:44.823307+03:00', 'end_date': '2024-11-18T17:23:09.952684+03:00', 'control_passed': None, 'pool_ids': '[\"2b05ce6b-b1d4-4e20-a097-e0ed09a8ad01\"]', 'price': 5.0, 'consistency': 0.0, 'quality': None, 'submitted_at': '2024-11-18T17:23:09.952684+03:00', 'skipped_at': None, 'expired_at': None, 'accepted_at': '2024-11-18T17:23:09.952684+03:00', 'rejected_at': None, 'started_at': '2024-11-18T17:22:44.823307+03:00', 'reviewer_id': None, 'reviewer_comment': None, 'OUTPUT:image': '/api/v0/storage/file/eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJhc3NpZ25tZW50X2lkIjoiNTZiNTU2YjUtZjEyYS00NDVjLTg0Y2UtMDA4NTYyZDBlNjg0IiwiYnVja2V0X2lkIjoiZGJlZGI1YmItMTZiYy00ZTA3LTliOTAtYTg4NWM4NjMzNmFlIiwiZXhwIjoxNzMxOTQwNjY0LCJmaWxlX2lkIjoiN2Y3ZWYzMTgtNjk3ZC00YmJlLWI5MTctNTQ1MWMwNTliNDkzIiwibWFya2VyX2lkIjoiNmFmMDIzN2YtYjZjZS00ZWY4LWEwZWEtNjc1OWEzYWE1MjgyIiwibWF4X2F0dGFjaG1lbnRzX3Blcl9hc3NpZ25tZW50Ijo1MH0.X-P_-6WDGayHy7RYbLhma4kIB0azB5ozYk9LjFMfN5EMx1eqB0M6HagudPlfkHSIlE7cdXT2JLr4biMoF-z5b9S0RMcELzb4yYZkHq-Wnt4yDOj-AIGulu-xJLu8lWUokFXyhgIaTtefyECLvCiu765GvyI9Dg8A9a9-C95a1S35DBpANMxVqtC-xZw7dBD6r3VuZU9vYRdUmq2TeSJDhkRP89xI2t4BNbwXd8-_rQ3CqkypUFqHahlk9fp0XPA2F-FRlxOv6qyWUIKR8wqq7uT3T4lb038O9_j0CT6oCBco2S7W66fHrsDkLOAzXsMp5_PRaJk5t-UqAnqPIvfZHO6JKuVTAF7ne7AH1k_KGFTDaa6KaGiseZ8N_7khq9svVhew3EFC3tIm1IVpFpKoL4QsPYnLmZiz_ZCJqjhSjLmFGTr0cEcDiX-fiPqesKKxf_ajPhOZMsAD8g7bUFlKkKk8CenznDDHs4BKvt9BbGMTUYriKIyCZH-WEZZ3VhKJHYHnBJJaCTFrVsPsH2nxyeTVkyTCsLmw1-Atv4PVHZE7fsbKlirCKg_ZQEF53HEeJ_Bx9ygcXshgz4MHhrFKoZVREeZTruQiwR1aUDjLGSqruf3qNfzRsKtIl721a8E54djid-GIZVYuVKA8eEw3CKKaF9ZSAeSHsCGXbQv_8aw', 'OUTPUT:classes': None}\n"
     ]
    }
   ],
   "source": [
    "directory = './' \n",
    "output_file = 'merged.json' \n",
    "\n",
    "merge_json_files(directory, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TorchImageDataset(\"./merged.json\",  \"./Resize/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37753"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():  \n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device       \n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            img,label = tuple(b.values())\n",
    "            img = to_device(img,self.device)\n",
    "            label = to_device(label, self.device)\n",
    "            yield {\"image\":img,\"label\":label}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, train_ratio=0.7, val_ratio=0.1):\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_data(dataset, train_ratio=0.2, val_ratio=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7550"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(DataLoader(train_data, batch_size=8,num_workers = 0), device=device)\n",
    "val_loader   = DeviceDataLoader(DataLoader(val_data, batch_size=8,num_workers = 0), device=device)\n",
    "test_loader   = DeviceDataLoader(DataLoader(test_data, batch_size=8,num_workers = 0), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMultilabel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes        \n",
    "        self.model = None\n",
    "        self.model = self.__efficientnet_b5()\n",
    "        \n",
    "    def __efficientnet_b5(self):\n",
    "        model = models.efficientnet_b5(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(2048,self.n_classes)\n",
    "        return model\n",
    "    def forward(self, input):\n",
    "        out = self.model(input)\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, dataloader_train, dataloader_valid, learningRate, num_epochs, device = 'cuda:0'):\n",
    "    logger = logging.getLogger('api_log')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(f'log_{learningRate}.txt')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.debug(f\"Start_train  lr={learningRate}\")\n",
    "    \n",
    "    # Установка оптимизатора с текущей скоростью обучения\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Цикл обучения\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for s in tqdm(dataloader_train, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
    "            if s is None:\n",
    "                continue\n",
    "            inputs = s['image']\n",
    "            labels = s['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Выходы модели\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            predicted = outputs\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            all_predictions.extend(predicted.detach().cpu().numpy())\n",
    "        # Средняя потеря за эпоху\n",
    "        epoch_loss = running_loss / len(dataloader_train)\n",
    "        # precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        # recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        # accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f_score = F_score(all_predictions, all_labels)\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, \"\n",
    "        #       f\"Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}, F_score: {f_score:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, F_score: {f_score:.4f}\")\n",
    "        \n",
    "        # logger.debug(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f},  F_score: {f_score:.4f}\")\n",
    "        validate_model(model, dataloader_valid, logger)    \n",
    "    # Сохранение модели\n",
    "    torch.save(model.state_dict(), f'model_{learningRate}.pth')\n",
    "    \n",
    "def validate_model(model, dataloader_valid, logger, device='cuda:0'):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(dataloader_valid, desc='Validation'):\n",
    "            if s is None:\n",
    "                continue\n",
    "            inputs = s['image']\n",
    "            labels = s['label']\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            all_predictions.extend(predicted.detach().cpu().numpy())\n",
    "\n",
    "    # precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    # recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    # accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f_score = F_score(all_predictions, all_labels)\n",
    "    # print(f\"Validation Results - Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    # logger.debug(f\"Validation Results - Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {accuracy:.4f}, F_score: {f_score:.4f}\")\n",
    "    logger.info(f\"Validation Results -  F_score: {f_score:.4f}\")\n",
    "\n",
    "\n",
    "def F_score(output, label, threshold=0.5, beta=1): #Calculate the accuracy of the model\n",
    "    if isinstance(output, list):\n",
    "        output = np.array(output)\n",
    "    if isinstance(label, list):\n",
    "        label = np.array(label)\n",
    "    print(label.shape, output.shape)\n",
    "    assert (output.shape == label.shape),'shape is different'\n",
    "    prob = output > threshold\n",
    "    label = label > threshold\n",
    "\n",
    "    TP = (prob == label).sum()\n",
    "    TN = (np.bitwise_not(prob) == np.bitwise_not(label)).sum()\n",
    "    FP = (prob == np.bitwise_not(label)).sum()\n",
    "    FN = (np.bitwise_not(prob) == label).sum()\n",
    "    print(TP, TN, FP, FN)\n",
    "    precision = np.mean(TP / (TP + FP + 1e-12))\n",
    "    recall = np.mean(TP / (TP + FN + 1e-12))\n",
    "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return F2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelMultilabel(15).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/30: 100%|██████████| 944/944 [17:46<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "94913 94913 18337 18337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [02:13<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12163 12163 1982 1982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "96934 96934 16316 16316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12079 12079 2066 2066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "97746 97746 15504 15504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12215 12215 1930 1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "98404 98404 14846 14846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12200 12200 1945 1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/30: 100%|██████████| 944/944 [03:21<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "98956 98956 14294 14294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12458 12458 1687 1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "99385 99385 13865 13865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12362 12362 1783 1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "99763 99763 13487 13487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12340 12340 1805 1805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "100049 100049 13201 13201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12447 12447 1698 1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "100519 100519 12731 12731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12418 12418 1727 1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "100633 100633 12617 12617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12420 12420 1725 1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "101173 101173 12077 12077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12348 12348 1797 1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "101037 101037 12213 12213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12598 12598 1547 1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "101613 101613 11637 11637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12569 12569 1576 1576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "101695 101695 11555 11555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12470 12470 1675 1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "101842 101842 11408 11408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12551 12551 1594 1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/30: 100%|██████████| 944/944 [03:20<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102037 102037 11213 11213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12583 12583 1562 1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102339 102339 10911 10911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12498 12498 1647 1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102307 102307 10943 10943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12611 12611 1534 1534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/30: 100%|██████████| 944/944 [03:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102222 102222 11028 11028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12532 12532 1613 1613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102658 102658 10592 10592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12590 12590 1555 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/30: 100%|██████████| 944/944 [03:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103034 103034 10216 10216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12600 12600 1545 1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/30: 100%|██████████| 944/944 [03:18<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "102888 102888 10362 10362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12566 12566 1579 1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/30: 100%|██████████| 944/944 [03:13<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103044 103044 10206 10206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12608 12608 1537 1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/30: 100%|██████████| 944/944 [03:12<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103327 103327 9923 9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:05<00:00, 19.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12590 12590 1555 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/30: 100%|██████████| 944/944 [03:11<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103409 103409 9841 9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12689 12689 1456 1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/30: 100%|██████████| 944/944 [03:12<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103539 103539 9711 9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12540 12540 1605 1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/30: 100%|██████████| 944/944 [03:18<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "103599 103599 9651 9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12679 12679 1466 1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/30: 100%|██████████| 944/944 [03:15<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "104234 104234 9016 9016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12638 12638 1507 1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/30: 100%|██████████| 944/944 [03:12<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "104361 104361 8889 8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12704 12704 1441 1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/30: 100%|██████████| 944/944 [03:16<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 15) (7550, 15)\n",
      "104437 104437 8813 8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 118/118 [00:06<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 15) (943, 15)\n",
      "12783 12783 1362 1362\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, 0.001, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_f1_score(model, filename, device='cuda:0', threshold=0.5):\n",
    "    test_dataset = TorchImageDataset(filename, \n",
    "                                    \"./Resize/\",\n",
    "                                   256)\n",
    "    valid_data = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        # if i != 1752:\n",
    "            valid_data.append(test_dataset[i] or \"\")\n",
    "    dataloader = DeviceDataLoader(DataLoader(valid_data, batch_size=8,num_workers = 0), device=device)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = (outputs > threshold).float()\n",
    "\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 283/283 [00:14<00:00, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7545780617299203)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_f1_score(model, \"pool_8_1_1.json\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3658/3658 [1:15:03<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = (outputs > 0.5).float()\n",
    "\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "all_predictions = np.vstack(all_predictions)\n",
    "all_labels = np.vstack(all_labels)\n",
    "    \n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')  \n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
